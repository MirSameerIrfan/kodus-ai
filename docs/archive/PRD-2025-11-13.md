# kodus-ai Context-OS - Product Requirements Document

**Author:** B Mad
**Date:** 2025-11-13
**Version:** 1.0

---

## Executive Summary

Context-OS elevates Kodus from a single AI code-review assistant to a shared intelligence
layer for every workflow. It captures reviewer telemetry, repository signals, product context,
and operational learnings into one governed source of truth so any agent, workflow, or tool
(MCP, CLI, API, IDE assistants) can reason with the same up-to-date organizational knowledge.

### What Makes This Special

Clareza total sobre a linhagem dos dados e vínculos contextuais: cada insight que um agente usa mostra de onde veio.
Context-OS funciona como um vínculo vivo entre código, discussões de produto e eventos operacionais,
permitindo auditoria rápida e construindo confiança entre engenharia, GTM e operações.

---

## Project Classification

**Technical Type:** developer_tool (Context-OS multi-interface)
**Domain:** enterprise devtool (cross-team context)
**Complexity:** Medium – brownfield integration with multi-surface delivery

Context-OS é uma devtool multi-superfície exposta via MCP, CLI, APIs e conectores de agentes.
Ela começa dentro do code-review da Kodus, mas foi desenhada para alimentar qualquer workflow (Cursor, Codex, Claude Code, automações internas) com o mesmo grafo de contexto governado.
O rollout precisa coordenar integrações brownfield com os pipelines atuais e liberar SDK/CLI para times externos.

### Domain Context

*(Nenhum briefing dedicado fornecido; aplicando boas práticas enterprise padrão para dados sensíveis.)*

---

## Success Criteria

1. Todo agente e workflow da Kodus (code review, MCP scripts, CLI, APIs públicas e integrações com Cursor/Codex/Claude Code) consegue solicitar contexto unificado sem passos manuais adicionais.
2. Cada insight entregue pelo Context-OS exibe origem, timestamp e nível de confiança, permitindo que reviewers e times de GTM/Operações validem rapidamente de onde veio a informação.
3. Camadas de segurança e governança garantem que nenhum dado sensível vaze entre times: RBAC aplicado, auditoria completa de consultas e alertas quando um vínculo de dados estiver inconsistente.
4. Lançar novos agentes passa a ser plug-and-play: basta declarar quais fontes precisam ser vinculadas ao grafo e o Context-OS fornece o material sem duplicar pipelines.
---

## Product Scope

### MVP - Minimum Viable Product

1. **Brownfield ingestion + normalization:** coletar sinais existentes do pipeline de code review (diffs, AST, comentários, configs) e consolidar num grafo de contexto versionado.
2. **Serviços de entrega MCP/CLI/API:** expor consultas de contexto em MCP (para IDEs/agents) e CLI interna para manual override, garantindo respostas determinísticas.
3. **Proveniência e confiança visível:** cada resultado inclui origem (repositório/arquivo/evento), timestamp e nível de confiança calculado.
4. **RBAC básico + auditoria:** amarrar acessos a times/projetos e registrar todo request de contexto.

### Growth Features (Post-MVP)

1. **Conectores além de código:** ingestão de briefs de produto, dados de suporte, notas de GTM e runbooks de operações.
2. **SDK público + webhooks:** permitir que ferramentas externas (Cursor, Codex, Claude Code) assinem eventos e enviem novos fragmentos de contexto.
3. **Painel de saúde do contexto:** dashboards mostrando lacunas, conflitos e alertas de dados sensíveis.
4. **Playbooks de agentes:** templates de workflows que aproveitam o Context-OS para tarefas como revisão, diagnóstico ou answer bots internos.

### Vision (Future)

1. **Context-Oriented Autonomy:** agentes profundos executam remediações ou análises ponta-a-ponta usando o grafo e devolvendo provas de origem.
2. **Marketplace de conectores confiáveis:** ecossistema onde parceiros adicionam fontes/transformers certificados com testes automáticos.
3. **Política dinâmica de governança:** engine declarativa decide retenção, mascaramento e propagação de contexto conforme regras por time/região.
4. **Observabilidade semântica:** queries naturais com explicações e simulações (“e se removêssemos esse vínculo?”) reforçando decisões estratégicas.

## Domain-Specific Requirements

Context-OS opera em ambiente enterprise com dados sensíveis (código privado, briefings internos, feedback de clientes). Portanto:
1. **Governança cruzada:** RBAC e políticas de dados alinhadas a times/projetos; dados de GTM não podem vazar para engineering sem permissão explícita.
2. **Proveniência obrigatória:** toda peça de contexto precisa manter hash/origem para auditoria (similar a SARIF + metadata Kokoon).
3. **Confiança operacional:** alertas quando vínculos estiverem quebrados ou com baixa confiança; registro de leituras/escritas no grafo.
4. **Conformidade corporativa:** atender requisitos padrão de enterprise SaaS (SOC2, ISO 27001) -> logs imutáveis, criptografia at-rest/in-transit.

This section shapes all functional and non-functional requirements below.

## Innovation & Novel Patterns

Context-OS trata contexto como um grafo vivo com proveniência assinada e entrega determinística para múltiplas superfícies (MCP, CLI, API, agentes).
Isso permite que qualquer agente consulte contextos confiáveis sem manter pipelines paralelos.
Além disso, o modelo de "context operating system" oferece políticas declarativas de governança, aproximando devtools de plataformas de dados corporativas.

### Validation Approach

1. **Shadow mode nas revisões:** rodar Context-OS em paralelo nas revisões atuais e comparar recomendações com reviewers humanos.
2. **Testes de proveniência:** cada insight precisa apontar para hash/commit/arquivo com checks automatizados.
3. **Pen-test de governança:** simular consultas indevidas e garantir bloqueios + alertas.
4. **Beta com agentes externos (Cursor/Codex/Claude Code):** validar se o SDK expõe contexto consistente sem customizações ad hoc.
---

## developer_tool (Context-OS multi-interface) Specific Requirements

### Deliverables per surface
1. **MCP Service** – implements `context.describe` + `context.query` commands returning structured provenance (source_id, commit_hash, confidence, ttl).
2. **CLI** – `kodus ctx pull <scope>` for humans; supports piping to jq/yq; includes `--explain` flag to show lineage tree.
3. **API Gateway** – REST + SSE for streaming context diffs; gRPC planned for agent-to-agent communication.
4. **Agent Connectors** – lightweight SDK adapters (TypeScript/Node first) for Cursor, Codex, Claude Code, GitHub Copilot Workspace.
5. **Workspace SDK** – `ContextClient` with caching hooks so user pipelines can embed the context graph locally when offline.

### Language & packaging
- Core services in TypeScript/Nest, gRPC bindings generated for Go/Python.
- CLI distributed via npm (`@kodus/ctx-cli`) and Homebrew tap.
- SDK published as `@kodus/context-os` with ES modules + type definitions.

### Installation / migration guidance
- Brownfield projects install the CLI and configure `context.config.yml` pointing to existing repos; bootstrap script ingests last 30 days of review activity.
- Provide migration playbook describing how to swap legacy context scripts to Context-OS queries.

### API Specification

```
GET /v1/context/snapshots?scope=<repo|team>&at=<commit>
  → Returns normalized facts with provenance array.

POST /v1/context/query
  Body: { "question": "...", "hints": ["repo:core", "type:review"] }
  → Streams reasoning steps + final answer with citations.

POST /v1/context/events
  → Agents push new facts (requires signed payload + schema validation).

SSE /v1/context/subscribe?topic=<scope>
  → Sends updates when new context links land for subscribed scopes.
```

### Authentication & Authorization

- OAuth 2.0 client credentials for services; CLI uses device code flow.
- All tokens mapped to RBAC roles (`reviewer`, `agent`, `observer`, `admin`).
- Fine-grained scopes: `ctx.read:repo`, `ctx.write:signals`, `ctx.admin:audit`.
- Attribute-based policies enforce separation between engineering/GTM/ops datasets.

### Platform Support

- MCP runtime spec >= 2024.09.
- CLI targets macOS/Linux; Windows via WSL.
- API served via HTTPS; requires HTTP/2 for streaming endpoints.
- IDE integrations tested on VS Code (Cursor), JetBrains (Code With Me), and web IDEs.

### Device Capabilities

Not applicable (server-side tooling only).

### Multi-Tenancy Architecture

- Logical tenants per customer with dedicated namespace in the context graph.
- Shared control plane manages schema migrations; data plane shards per tenant for isolation.
- Supports bring-your-own bucket for long-term storage (S3, GCS).

### Permissions & Roles

| Role      | Read Repo Context | Write Signals | Subscribe SSE | Manage Policies |
|-----------|------------------|---------------|---------------|-----------------|
| Reviewer  | ✅ scoped         | ❌             | ✅ scoped      | ❌               |
| Agent     | ✅ scoped         | ✅ via contract| ✅ scoped      | ❌               |
| Observer  | ✅ read-only logs | ❌            | ❌            | ❌               |
| Admin     | ✅ all tenants    | ✅             | ✅             | ✅               |

## User Experience Principles

### Key Interactions

1. `context.describe scope=repo` → retorna resumo + fontes.
2. `context.query question="Por que esse PR recebeu alerta?"` → devolve raciocínio com citações.
3. `ctx-cli diff --since <commit>` → mostra o que mudou no grafo.
4. IDE agent usa MCP `context.query` antes de sugerir código.

---

## Functional Requirements

1. **FR-1 Context Graph Ingestion** – System ingests code review artifacts (diffs, AST, PR comments, config) within 5 minutes of availability and stores them as signed vertices with provenance metadata (source, commit, timestamp, confidence).
2. **FR-2 Provenance API** – Every query response must include a provenance array referencing stored vertices so users/agents can trace back to original files or documents.
3. **FR-3 MCP Interface** – Provide `context.describe` and `context.query` commands that accept scopes (repo/team/file) and optional hints, returning JSON with lineage + suggested follow-up prompts.
4. **FR-4 CLI Tooling** – `kodus ctx pull|diff|explain` commands retrieve context locally, support piping, and exit non-zero when provenance is incomplete.
5. **FR-5 Event Push** – Agents can push new signals via `/v1/context/events`; payloads validated against schema registry with rejection reasons surfaced in audit logs.
6. **FR-6 Access Control** – All read/write operations checked against RBAC + attribute policies; unauthorized access attempts logged and alerted.
7. **FR-7 Subscription/Streaming** – SSE endpoint broadcasts context updates per scope; clients can resume with last-event-id to avoid missed data.
8. **FR-8 Governance Dashboard** – Provide API/CLI endpoints exposing audit trails, broken links, and confidence anomalies so ops can intervene quickly.

---

## Non-Functional Requirements

### Performance

- MCP queries must return first byte < 800ms for scoped queries (repo/team) and < 2s for org-wide queries at P95.
- SSE stream can fan-out to 5k concurrent subscribers per tenant without dropping events.

### Security

- All context data encrypted at rest (AES-256) and in transit (TLS 1.3); provenance hashes signed.
- RBAC + ABAC enforced centrally; audit log immutable (append-only) with 90-day retention minimum.
- CLI and SDK enforce signed requests; reject unsigned payloads automatically.

### Scalability

- Graph storage must handle 10M vertices per tenant with horizontal partitioning.
- Ingestion pipeline scales via queue-based fan-out; adding new sources should not require downtime.

### Accessibility

Not applicable (no user-facing UI in MVP).

### Integration

- Standard schema registry for signals (`context.schema.yaml`) – agents must conform to publish new types.
- Webhook retry/backoff semantics documented so external tools can rely on deliveries.

---

## Reference Architecture (Best-Practice Draft)

1. **Ingress Layer (Collectors)**
   - GitHub/GitLab webhooks, AST analyzers, pipeline hooks push events to `context-ingest-queue`.
   - Normalizers convert raw payloads into `ContextFact` records with source metadata.
2. **Context Graph Service**
   - Backed by document store + graph index (Neo4j or Neptune) to support lineage queries.
   - Writes versioned snapshots and maintains confidence scores per edge.
3. **Governance & Policy Engine**
   - Evaluates RBAC/ABAC policies before read/write.
   - Emits audit logs to immutable storage (e.g., AWS QLDB / OpenSearch with WORM).
4. **Delivery Layer**
   - **MCP Service** for IDE agents.
   - **Context API Gateway** (REST/gRPC/SSE) with caching tier.
   - **CLI + SDK** hitting gateway with signed requests.
5. **Observability**
   - Telemetry pipeline measuring ingestion latency, query P95, provenance completeness.
   - Health dashboard surfaces broken links, stale data, policy violations.

```
Sources → Ingest Queue → Normalizers → Context Graph Store
                                    ↘ Audit / Policy Engine ↗
Context Graph ←→ API Gateway / MCP / CLI / SDK
                       ↘ Observability + Dashboards
```

---

## Example Types & Interfaces

```ts
export type ConfidenceLevel = 'high' | 'medium' | 'low';

export interface ContextFact {
  id: string;                // UUID
  scope: string;             // e.g., repo:core, team:review
  type: 'code' | 'product' | 'runbook' | 'support';
  payload: Record<string, unknown>;
  source: {
    system: 'github' | 'linear' | 'pagerduty' | string;
    reference: string;       // commit hash, ticket id, doc URL
    collectedAt: string;     // ISO timestamp
  };
  confidence: ConfidenceLevel;
  tags?: string[];
}

export interface ProvenanceEdge {
  factId: string;
  justification: string;
}

export interface ContextQueryRequest {
  scope: string;
  question?: string;
  hints?: string[];
  maxFacts?: number;
}

export interface ContextQueryResponse {
  answer: string;
  facts: ContextFact[];
  provenance: ProvenanceEdge[];
  generatedAt: string;
}
```

These types back both the MCP commands and the REST/gRPC APIs, ensuring every agent/tooling integration shares the same contract.

---

## Implementation Planning

### Epic Breakdown Required

Requirements must be decomposed into epics and bite-sized stories (200k context limit).

**Next Step:** Run `workflow epics-stories` to create the implementation breakdown.

### PRD Summary Snapshot

- **Vision:** Context-OS como camada única de inteligência e proveniência para toda a Kodus.
- **Success:** contexto unificado, proveniência explícita, RBAC sólido, agentes plug-and-play.
- **Scope:** MVP foca ingestão brownfield + entrega MCP/CLI/API; growth adiciona conectores cross-org; visão habilita autonomia orientada a contexto.
- **Requirements:** 8 FRs principais + NFRs de performance, segurança, escala, integração.
- **Special Considerations:** Governança enterprise, marketplace de conectores e observabilidade semântica.

**Project Track:** BMad Method  
**Epic Seeds (para próxima fase):**
1. Core Ingestion & Graph Service
2. Delivery Interfaces (MCP/CLI/API)
3. Governance & Audit Plane
4. Multi-source Expansion (GTM/Support)
5. Observability & Health Dashboard

---

## References

- Product Brief: None – product brief ainda não foi fornecido (construindo ao vivo)
- Domain Brief: None – sem domain brief dedicado
- Research: None – nenhuma pesquisa carregada (usar discovery em tempo real)

---

## Next Steps

1. **Epic & Story Breakdown** - Run: `workflow epics-stories`
2. **UX Design** (if UI) - Run: `workflow ux-design`
3. **Architecture** - Run: `workflow create-architecture`

---

_This PRD captures the essence of kodus-ai Context-OS - Um Context-OS governado que unifica sinais de todo o ciclo de desenvolvimento e os entrega com proveniência para qualquer agente ou workflow._

_Created through collaborative discovery between B Mad and AI facilitator._
